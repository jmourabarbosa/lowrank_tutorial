{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jmourabarbosa/lowrank_tutorial/blob/main/tutorial2.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### uncomment this if running in google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/jmourabarbosa/lowrank_tutorial/\n",
    "import sys\n",
    "sys.path.append('lowrank_tutorial/')\n",
    "root_path = \"lowrank_tutorial/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from low_rank_rnns.modules import LowRankRNN,train\n",
    "from low_rank_rnns import  rdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of this tutorial:\n",
    "\n",
    "1. Train (or import a trained) network on the random-dots task\n",
    "2. Reverse egineer the solution found with gradient descent:\n",
    "\n",
    "    2.1 Solve two equations, instead N: $\\kappa$ and input.\n",
    "\n",
    "    2.2 Sample a different network that lose all neuron specificity but keeps the geometric arrangement between the network vectors. This network solves the task in the same way.\n",
    "\n",
    "This tutorial is heavily based on the paper and code provided in https://github.com/adrian-valente/populations_paper_code/.\n",
    "\n",
    "For the porpuse of this tutorial, we will try to abstract completely the fine art of training RNNs and focus on its analyses.\n",
    "You can find many tutorial on how to train RNNs online. I recommend you take a look at neurogym: https://neurogym.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 1.***\n",
    "\n",
    "The following code initializes a rank 1 RNN and generates 1000 trials from the random dots task.\n",
    "\n",
    "* load and plot 2 trials (coherence 1 and -1) from random dots task\n",
    "* run the network with these 2 trials\n",
    "* check single neurons and mean activity of the network\n",
    "* extract the important vectors of the RNN (m, n, wi, wo) - what is their relationship?\n",
    "* What is the output (i.e. of projection of ativity onto wo) of the untrained network?\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/jmourabarbosa/lowrank_tutorial/blob/main/figures2/1.png?raw=1\" alt=\"drawing\" width=\"600\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, y_train, mask_train, x_val, y_val, mask_val = rdm.generate_rdm_data(1000)\n",
    "time = np.arange(x_train.shape[1]) * rdm.deltaT / 1000\n",
    "stim_idx = np.zeros_like(time)\n",
    "stim_idx[rdm.fixation_duration_discrete:rdm.stimulus_end] = 1\n",
    "\n",
    "net = LowRankRNN(input_size=1, hidden_size=512, output_size=1, noise_std = 5e-2, alpha=0.2, train_wi=False, train_wo=False, rank=1)\n",
    "\n",
    "m = net.m[:,0].detach().numpy()\n",
    "n = net.n[:,0].detach().numpy()\n",
    "wi = net.wi_full[0].detach().numpy()\n",
    "wo = net.wo_full[:,0].detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = ['indianred','royalblue']\n",
    "\n",
    "# Generate two trials and network response\n",
    "x1, y1, mask1, _, _, _ = rdm.generate_rdm_data(1, coherences=[-1], fraction_validation_trials=0.)\n",
    "x2, y2, mask2, _, _, _ = rdm.generate_rdm_data(1, coherences=[1], fraction_validation_trials=0.)\n",
    "output1, hidden_states1 = net.forward(x1, return_dynamics=True)\n",
    "output2, hidden_states2 = net.forward(x2, return_dynamics=True)\n",
    "\n",
    "output1 = output1.detach().squeeze().numpy()\n",
    "output2 = output2.detach().squeeze().numpy()\n",
    "hidden_states1 = hidden_states1.detach().squeeze().numpy()\n",
    "hidden_states2 = hidden_states2.detach().squeeze().numpy()\n",
    "x1 = x1.squeeze().numpy()\n",
    "x2 = x2.squeeze().numpy()\n",
    "y1 = y1.squeeze().numpy()\n",
    "y2 = y2.squeeze().numpy()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.subplot(3,1,1)\n",
    "plt.title(\"inputs\")\n",
    "(...)\n",
    "plt.plot(time,np.zeros_like(time), \"k--\", lw=1)\n",
    "plt.plot(time[stim_idx==1], np.ones_like(time[stim_idx==1]) * 0.5, \"k\", lw=3)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.title(\"single neurons\")\n",
    "(...)\n",
    "plt.ylim(-2, 2)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.title(\"network outputut\")\n",
    "(...)\n",
    "plt.plot(time[-1], y1[-1], \"o\", ms=10,c=colors[0], lw=3)\n",
    "plt.plot(time[-1], y2[-1], \"o\", ms=10,c=colors[1], lw=3, alpha=.8)\n",
    "plt.plot(time,np.zeros_like(time), \"k--\", lw=1)\n",
    "plt.ylim(-2, 2)\n",
    "plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 2.***\n",
    "\n",
    "* train a network (or load a trained network) network on the task\n",
    "* check the network output and single neuron activity\n",
    "* what changed in the network vectors? (hint: look at their geometric relationship, ie the overlaps)\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/jmourabarbosa/lowrank_tutorial/blob/main/figures2/2.png?raw=1\" alt=\"drawing\" width=\"600\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(net, x_train, y_train, mask_train, lr=5e-3, n_epochs=20, batch_size=32, keep_best=True, cuda=True)\n",
    "net.load_state_dict(torch.load(root_path+f'rdm_lr_fig2.pt', map_location='cpu'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.***\n",
    "\n",
    "The following code initializes 4 different inputs, each with difference coherence levels.\n",
    "\n",
    "* run the trained network with these 4 inputs and save the hidden units activity\n",
    "* project the hidden states of each of these trials into the m-I plance (hint: remember to orthogonlize I with respect to m)\n",
    "* (bonus): project the n and I vectors on this plane.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/jmourabarbosa/lowrank_tutorial/blob/main/figures2/3.png?raw=1\" alt=\"drawing\" width=\"600\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net.noise_std = 0.\n",
    "\n",
    "inputs = torch.zeros((4, rdm.total_duration + 100, 1))\n",
    "inputs[0, rdm.fixation_duration_discrete:rdm.stimulus_end] += .2 * rdm.SCALE\n",
    "inputs[1, rdm.fixation_duration_discrete:rdm.stimulus_end] += .6 * rdm.SCALE\n",
    "inputs[2, rdm.fixation_duration_discrete:rdm.stimulus_end] += -.2 * rdm.SCALE\n",
    "inputs[3, rdm.fixation_duration_discrete:rdm.stimulus_end] += -.6 * rdm.SCALE\n",
    "\n",
    "\n",
    "proj1 = (...)\n",
    "proj2 = (...)\n",
    "\n",
    "plt.plot(proj1[1], proj2[1], lw=3, label='+0.6', c=colors[0])\n",
    "plt.plot(proj1[0], proj2[0], lw=3, label='+0.2', c=colors[0], alpha=.5)\n",
    "plt.plot(proj1[2], proj2[2], lw=3, label='-0.2', c=colors[1], alpha=.5)\n",
    "plt.plot(proj1[3], proj2[3], lw=3, label='-0.6', c=colors[1])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.quiver(0, 0, 1, 0, scale=8, color='green')\n",
    "plt.quiver(0, 0,(...), scale=8, color='purple')\n",
    "plt.quiver(0, 0, (...), scale=5, color='goldenrod')\n",
    "plt.text(-.05, .01, '$\\mathbf{I}$', size=25, color='purple')\n",
    "plt.text(.14, -.005, '$\\mathbf{m}$', size=25, color='green')\n",
    "plt.text(.15, .006, '$\\mathbf{n}$', size=25, color='goldenrod')\n",
    "plt.legend(bbox_to_anchor=(1., .5), loc='center left', labelspacing=.3, handlelength=.8, title='Input', \n",
    "          fontsize=18, title_fontsize=20, frameon=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 4.***\n",
    "Recall yesterday's tutorial equation for $\\kappa$:\n",
    "\n",
    "$$ \\dot \\kappa = -k + \\frac{1}{N}n^T\\phi(\\kappa m + v I) \\; \\; \\; \\small (2)$$\n",
    "$$ \\dot v = -v + u I$$\n",
    "\n",
    "\n",
    "* Using what you learned from yesterday's tutorial, potentially using the same code, integrate $\\dot \\kappa$ and $\\dot v$.\n",
    "* How good is the match between the network activity and this 2D system?\n",
    "* (bonus) How could you recover the high-dimensional system (n=512) from this low-d (n=2)?\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/jmourabarbosa/lowrank_tutorial/blob/main/figures2/4.png?raw=1\" alt=\"drawing\" width=\"600\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt =  net.alpha\n",
    "\n",
    "\n",
    "k_ = np.zeros([4])\n",
    "v_ = np.zeros([4])\n",
    "\n",
    "K = np.zeros((4,rdm.total_duration + 100))\n",
    "V = np.zeros((4,rdm.total_duration + 100))\n",
    "\n",
    "for ti in range(rdm.total_duration + 100):\n",
    "(...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(proj1[1], proj2[1], lw=3, label=\"proj $m$\",c=colors[0])\n",
    "plt.plot(proj1[0], proj2[0], lw=3, c=colors[0], alpha=.5)\n",
    "plt.plot(proj1[2], proj2[2], lw=3, c=colors[1], alpha=.5)\n",
    "plt.plot(proj1[3], proj2[3], lw=3, c=colors[1])\n",
    "\n",
    "plt.plot(K[1], V[1], \"--\",lw=3, label=\"$\\kappa$\", c=colors[0])\n",
    "plt.plot(K[0], V[0], \"--\",lw=3, c=colors[0], alpha=.5)\n",
    "plt.plot(K[2], V[2], \"--\",lw=3, c=colors[1], alpha=.5)\n",
    "plt.plot(K[3], V[3], \"--\",lw=3, c=colors[1])\n",
    "plt.axis('off')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1., .5), loc='center left', labelspacing=.3, handlelength=.8,\n",
    "          fontsize=18, title_fontsize=20, frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 5.***\n",
    "\n",
    "Recall yesterday's lecture first exercise:\n",
    "\n",
    "\"An important insight of the low-rank RNN theory is that the alignment between the vectors definning the RNN (in this case $m,n,I$) fully determine the dynamics performed by the network.\"\n",
    "\n",
    "For the final exercise we will do an empirical (but very strong!) demonstration of this statement. \n",
    "\n",
    "Specifically, we will sample a new network that keeps the geometric relationshops of its vectors, but mixes everything else.\n",
    "\n",
    "* calculate the mean and covariances of all the network vectors (m,n,wi,wo)\n",
    "* sample a new set of vectors from a multivariance gaussian with that mean and covariance (hint: use np.random.multivariate_normal)\n",
    "* what is the relationship of these new vectors relative to the new ones? \n",
    "* use these new vectors to initialize a new network (either using the helper LowRankRNN (hint: check help(LowRankRNN) to know how to initialize the network with these vectors) or integrating it with euler as we did yesterday)\n",
    "* perform analyses to convice yourself this network also solves the task\n",
    "* (bonus) calculate psychometric curves for each network \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/jmourabarbosa/lowrank_tutorial/blob/main/figures2/5.png?raw=1\" alt=\"drawing\" width=\"600\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vects = np.array([m,n,wi,wo])\n",
    "means = (...)\n",
    "overlaps = (..)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decision(output):\n",
    "    decisions = (torch.sign(output[:, rdm.response_begin:, 0].mean(dim=1).squeeze()) + 1) // 2\n",
    "    return decisions\n",
    "\n",
    "cohs = [-2, -1, -.5, -.2, 0., .2, .5, 1, 2]\n",
    "\n",
    "        \n",
    "ax.plot(np.array(cohs), probs_res, c='gray', lw=3, label='trained', marker='o')\n",
    "ax.plot(np.array(cohs), probs_trained, c='seagreen', lw=3, label='resamp.', marker='o')\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_yticks([0., .5, 1.])\n",
    "ax.set_xlabel('input')\n",
    "ax.set_ylabel('positive choices')\n",
    "ax.legend(frameon=False, fontsize=19, bbox_to_anchor=(.8, .8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: train a network in a more complex task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
